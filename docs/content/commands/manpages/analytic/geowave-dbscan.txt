//:= geowave-analytic-dbscan(1)
:doctype: manpage

[[analytic-dbscan-name]]
==== NAME

geowave-analytic-dbscan - Density-Based Scanner

[[analytic-dbscan-synopsis]]
==== SYNOPSIS

  geowave analytic dbscan [options] <storename>

[[analytic-dbscan-description]]
==== DESCRIPTION

This command runs a density based scanner analytic on GeoWave data.

[[analytic-dbscan-options]]
==== OPTIONS

*-conf, --mapReduceConfigFile* _<file>_::
  MapReduce configuration file.

*$$*$$ -hdfsbase, --mapReduceHdfsBaseDir* _<path>_::
  Fully qualified path to the base directory in HDFS.

*$$*$$ -jobtracker, --mapReduceJobtrackerHostPort* _<host>_::
  [REQUIRED (or `-resourceman`)] Hadoop job tracker hostname and port in the format `hostname:port`.

*$$*$$ -resourceman, --mapReduceYarnResourceManager* _<host>_::
  [REQUIRED (or `-jobtracker`)] Yarn resource manager hostname and port in the format `hostname:port`.
  
*-hdfs, --mapReduceHdfsHostPort* _<host>_::
  HDFS hostname and port in the format `hostname:port`.

*--cdf, --commonDistanceFunctionClass* _<class>_::
  Distance function class that implements `org.locationtech.geowave.analytics.distance.DistanceFn`.
  
*$$*$$ --query.typeNames* _<types>_::
  The comma-separated list of types to query; by default all types are used.

*--query.auth* _<auths>_::
  The comma-separated list of authorizations used during extract; by default all authorizations are used.

*--query.index* _<index>_::
  The specific index to query; by default one is chosen for each adapter.
  
*$$*$$ -emx, --extractMaxInputSplit* _<size>_::
  Maximum HDFS input split size.

*$$*$$ -emn, --extractMinInputSplit* _<size>_::
  Minimum HDFS input split size.

*-eq, --extractQuery* _<query>_::
  Query
  
*-ofc, --outputOutputFormat* _<class>_::
  Output format class.
  
*-ifc, --inputFormatClass* _<class>_::
  Input format class.

*-orc, --outputReducerCount* _<count>_::
  Number of reducers For output.

*$$*$$ -cmi, --clusteringMaxIterations* _<count>_::
  Maximum number of iterations when finding optimal clusters.

*$$*$$ -cms, --clusteringMinimumSize* _<size>_::
  Minimum cluster size.

*$$*$$ -pmd, --partitionMaxDistance* _<distance>_::
  Maximum partition distance.

*-b, --globalBatchId* _<id>_::
  Batch ID.

*-hdt, --hullDataTypeId* _<id>_::
  Data Type ID for a centroid item.

*-hpe, --hullProjectionClass* _<class>_::
  Class to project on to 2D space. Implements `org.locationtech.geowave.analytics.tools.Projection`.

*-ons, --outputDataNamespaceUri* _<namespace>_::
  Output namespace for objects that will be written to GeoWave.

*-odt, --outputDataTypeId* _<id>_::
  Output Data ID assigned to objects that will be written to GeoWave.

*-oop, --outputHdfsOutputPath* _<path>_::
  Output HDFS file path.

*-oid, --outputIndexId* _<index>_::
  Output index for objects that will be written to GeoWave.

*-pdt, --partitionDistanceThresholds* _<thresholds>_::
  Comma separated list of distance thresholds, per dimension.

*-pdu, --partitionGeometricDistanceUnit* _<unit>_::
  Geometric distance unit (m=meters,km=kilometers, see symbols for javax.units.BaseUnit).

*-pms, --partitionMaxMemberSelection* _<count>_::
  Maximum number of members selected from a partition.

*-pdr, --partitionPartitionDecreaseRate* _<rate>_::
  Rate of decrease for precision(within (0,1]).

*-pp, --partitionPartitionPrecision* _<precision>_::
  Partition precision.

*-pc, --partitionPartitionerClass* _<class>_::
  Index identifier for centroids.

*-psp, --partitionSecondaryPartitionerClass* _<class>_::
  Perform secondary partitioning with the provided class.

[[analytic-dbscan-examples]]
==== EXAMPLES

Run through 5 max iterations (`-cmi`), with max distance between points as 10 meters (`-cms`), min HDFS input split is 2 (`-emn`), max HDFS input split is 6 (`-emx`), max search distance is 1000 meters (`-pmd`), reducer count is 4 (`-orc`), the HDFS IPC port is `localhost:53000` (`-hdfs`), the yarn job tracker is at `localhost:8032` (`-jobtracker`), the temporary files needed by this job are stored in `hdfs:/host:port//user/rwgdrummer` (`-hdfsbase`), the data type used is `gpxpoint` (`-query.typeNames`), and the data store connection parameters are loaded from `my_store`.

  geowave analytic dbscan -cmi 5 -cms 10 -emn 2 -emx 6 -pmd 1000 -orc 4 -hdfs localhost:53000 -jobtracker localhost:8032 -hdfsbase /user/rwgdrummer --query.typeNames gpxpoint my_store

[[analytic-dbscan-execution]]
==== EXECUTION

DBSCAN uses GeoWaveInputFormat to load data from GeoWave into HDFS. You can use the extract query parameter to limit the records used in the analytic.

It iteratively calls Nearest Neighbor to execute a sequence of concave hulls. The hulls are saved into sequence files  written to a temporary HDFS directory, and then read in again for the next DBSCAN iteration. 

After completion, the data is written back from HDFS to Accumulo using a job called the "input load runner".

